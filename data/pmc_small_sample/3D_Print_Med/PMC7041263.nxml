<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">3D Print Med</journal-id><journal-id journal-id-type="iso-abbrev">3D Print Med</journal-id><journal-title-group><journal-title>3D Printing in Medicine</journal-title></journal-title-group><issn pub-type="epub">2365-6271</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">32095949</article-id><article-id pub-id-type="pmc">7041263</article-id><article-id pub-id-type="publisher-id">59</article-id><article-id pub-id-type="doi">10.1186/s41205-020-00059-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Technical Note</subject></subj-group></article-categories><title-group><article-title>Non-contact visual control of personalized hand prostheses/exoskeletons by tracking using augmented reality glasses</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5994-3810</contrib-id><name><surname>Hazubski</surname><given-names>Simon</given-names></name><address><email>simon.hazubski@hs-offenburg.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Hoppe</surname><given-names>Harald</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Otte</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label>Laboratory of Computer Assisted Medicine, Offenburg University, Badstr. 24, D-77652 Offenburg, Germany </aff><aff id="Aff2"><label>2</label>Laboratory of NeuroScience, Division of Medical Engineering, Department of Electrical Engineering, Medical Engineering and Computer Science, Offenburg University, Badstr. 24, D-77652 Offenburg, Germany </aff></contrib-group><pub-date pub-type="epub"><day>24</day><month>2</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>24</day><month>2</month><year>2020</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2020</year></pub-date><volume>6</volume><elocation-id>6</elocation-id><history><date date-type="received"><day>9</day><month>9</month><year>2019</year></date><date date-type="accepted"><day>18</day><month>2</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">A new concept for robust non-invasive optical activation of motorized hand prostheses by simple and non-contact commands is presented. In addition, a novel approach for aiding hand amputees is shown, outlining significant progress in thinking worth testing. In this, personalized 3D-printed artificial flexible hands are combined with commercially available motorized exoskeletons, as they are used e.g. in tetraplegics.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Personalized prostheses</kwd><kwd>3D-print</kwd><kwd>Visual control</kwd><kwd>Augmented reality</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003542</institution-id><institution>Ministerium f&#x000fc;r Wissenschaft, Forschung und Kunst Baden-W&#x000fc;rttemberg</institution></institution-wrap></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p id="Par2">Current approaches to controlling electrical neuroprostheses are based on measuring electromyography (EMG) signals of still existing muscles or using brain-machine or nerve-machine interface concepts to evaluate neuronal patterns and derive commands for the prosthesis from brain arrays, intrafascicular nerve electrodes, or combined electroencephalography/electrooculography (EEG/EOG) devices [<xref ref-type="bibr" rid="CR1">1</xref>]. These neuroprosthetic concepts are intriguing and developing rapidly, albeit some of them are invasive or discomforting for the user and may not always reflect his or her wishes seeking for a smart but as simple as possible prosthesis, which can be attached, used, and controlled independently [<xref ref-type="bibr" rid="CR2">2</xref>]. Some encouraging non-invasive and low-cost approaches have been developed, but most of them still require extended support, e.g. when electrodes of a non-invasive EEG/EOG system have to be attached.</p><p id="Par3">In our new concept (Fig. <xref rid="Fig1" ref-type="fig">1</xref>), the only interface to the patient is with augmented reality (AR) technology by optical see-through glasses (OSTG), equipped with a front camera. The hand prosthesis may be any active motorized hand prosthesis or robotic arm. Markers are attached on the prosthetic hand, which may be (infrared) light emitting diodes (LEDs) or glued points. If the camera recognizes the markers, the transformation from the coordinate system of the hand prosthesis to the coordinate system of the AR glasses can be determined by means of an already developed algorithm [<xref ref-type="bibr" rid="CR3">3</xref>]. As a result, the condition is created to evaluate the viewing direction of the viewer. The direction of view is the orientation of the glasses to the hand prosthesis, not the direction of the eyes. On or near the prosthesis is a virtual control panel. This control panel can either be permanently superimposed by the AR glasses or only be superimposed when the viewing direction approaches this region. In the simplest case, the control panel could contain virtual &#x0201c;push buttons&#x0201d; that trigger when the viewer&#x02019;s gaze persists for more than one second, for example. In addition, the user could also execute commands in the form of defined minimal movements of the viewing direction.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Schematic representation of the visual control for the hand prosthesis</p></caption><graphic xlink:href="41205_2020_59_Fig1_HTML" id="MO1"/></fig></p><p id="Par4">The evaluation of the commands can be efficiently implemented by means of an artificial neural network. Among others, following commands are conceivable:
<list list-type="order"><list-item><p id="Par5">Viewing direction of the observer goes through command field from upper left corner to lower right corner &#x02192; Prosthesis closes hand completely</p></list-item><list-item><p id="Par6">Movement in the opposite direction as under 1. &#x02192; Prosthesis opens hand completely</p></list-item><list-item><p id="Par7">Viewing direction of the observer passes through command field from upper left corner to lower right corner only partially &#x02192; Prosthesis closes hand in half</p></list-item><list-item><p id="Par8">Partial movement in the opposite direction as in 3. &#x02192; Prosthesis opens hand in half</p></list-item><list-item><p id="Par9">Circular movement of the line of vision &#x02192; Thumb moves</p></list-item></list></p><p id="Par10">The commands can be customized according to the functions of the prosthetic hand and the preferences of the user. For instance, in hand prostheses which automatically switch off at a certain contact pressure, only the on/off command suffices. The user receives feedback, for example, by a trace of his/her movements of the viewing direction on the control panel or by a virtually displayed control panel. Optionally, the OSTG can be equipped with a camera for eye gaze tracking. The additional use of eye tracking can improve the accuracy of the recognition of the commands in the virtual control panel, as the user&#x02019;s current viewing direction can be determined from the eye position, which may be important for certain neurological indications such as severe traumatic brain injury, stroke, amyotrophic lateral sclerosis, or other conditions, in which head movement is limited or completely impossible.</p><p id="Par11">Non-invasive fully-independent approaches to aid amputees or tetraplegics outside of the laboratory environment are on its way [<xref ref-type="bibr" rid="CR4">4</xref>]. Non-contact visual control of neuroprostheses by AR glasses could be an alternative to conventional EMG- or EEG/EOG-driven concepts worth looking for. Up to now, AR glasses have only been included into current EMG or EEG/EOG intention detection methods as an add-on to the concept [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. Solely using AR glasses&#x02013;without any other complex signal detection by EMG, EOG or EEG&#x02013;to control the prosthesis or robot hand is new and may simplify usability of prosthetic devices for the patient in the near future.</p><p id="Par12">Low-cost 3D-printed motorized hands, controlled by OSTG and driven by energy-efficient neural network platforms could have an impact. Another step towards personalizing hand prostheses in amputees could be to use an easy-to-handle and inexpensive standard motorized exoskeleton instead of a sophisticated motorized hand prosthesis and to replace the missing hand with a LASER scan of the healthy hand (if available) or a volunteer&#x02019;s hand. If a healthy hand can be scanned, the data could be mirrored to the other side of the person to replace the missing hand and 3D printed using flexible and lightweight polymer material. This personalized hand replacement could then be equipped with the exoskeleton in the same way as e.g. a tetraplegic&#x02019;s hand.</p></body><back><fn-group><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>Not applicable.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>AO and SH wrote the text and designed the figure. Every author listed above has been involved in conceptual design. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>The article processing charge was funded by the Baden-W&#x000fc;rttemberg Ministry of Science, Research and Culture and Offenburg University in the funding programme Open Access Publishing.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>Not applicable.</p></notes><notes><title>Ethics approval and consent to participate</title><p id="Par13">Not applicable.</p></notes><notes><title>Consent for publication</title><p id="Par14">Not applicable.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par15">S.H., H.H., and A.O. are inventors on patent application DE 10 2019 108 670.1 submitted to the German Patent and Trade Mark Office (DPMA) by Offenburg University that covers the invention of the presented new method of augmented reality glasses for controlling hand prostheses.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borton</surname><given-names>D</given-names></name><name><surname>Micera</surname><given-names>S</given-names></name><name><surname>Mill&#x000e1;n</surname><given-names>J d R</given-names></name><name><surname>Courtine</surname><given-names>G</given-names></name></person-group><article-title>Personalized neuroprosthetics</article-title><source>Sci Transl Med</source><year>2013</year><volume>5</volume><fpage>210rv2</fpage><pub-id pub-id-type="doi">10.1126/scitranslmed.3005968</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otte</surname><given-names>A</given-names></name></person-group><article-title>Smart neuroprosthetics becoming smarter, but not for everyone?</article-title><source>EClinicalMedicine</source><year>2018</year><volume>2&#x02013;3</volume><fpage>11</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.eclinm.2018.08.005</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">S. Strzeletz, S. Hazubski, J. L. Moctezuma, H. Hoppe, Peer-to-Peer-Navigation in der computerassistierten Chirurgie, in <italic>Tagungsband der 17. Jahrestagung der Deutschen Gesellschaft f&#x000fc;r Computer- und Roboterassistierte Chirurgie (CURAC)</italic>, 13 to 15 September 2018, Hrsg. T. Neumuth, A. Melzer, C. Chalopin, ISBN 978&#x02013;3&#x02013;00-060786-8, pp. 119&#x02013;124.</mixed-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soekadar</surname><given-names>SR</given-names></name><name><surname>Witkowski</surname><given-names>M</given-names></name><name><surname>G&#x000f3;mez</surname><given-names>C</given-names></name><name><surname>Opisso</surname><given-names>E</given-names></name><name><surname>Medina</surname><given-names>J</given-names></name><name><surname>Cortese</surname><given-names>M</given-names></name><name><surname>Cempini</surname><given-names>M</given-names></name><name><surname>Carrozza</surname><given-names>MC</given-names></name><name><surname>Cohen</surname><given-names>LG</given-names></name><name><surname>Birbaumer</surname><given-names>N</given-names></name><name><surname>Vitiello</surname><given-names>N</given-names></name></person-group><article-title>Hybrid EEG/EOG-based brain/neural hand exoskeleton restores fully independent daily living activities after quadriplegia</article-title><source>Sci Robot</source><year>2016</year><volume>1</volume><fpage>eaag3296</fpage><pub-id pub-id-type="doi">10.1126/scirobotics.aag3296</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Kang</surname><given-names>BB</given-names></name><name><surname>Kim</surname><given-names>KB</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Ha</surname><given-names>J</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Jo</surname><given-names>S</given-names></name></person-group><article-title>Eyes are faster than hands: A soft wearable robot learns user intention from the egocentric view</article-title><source>Sci Robot</source><year>2019</year><volume>4</volume><fpage>eaav2949</fpage><pub-id pub-id-type="doi">10.1126/scirobotics.aav2949</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMullen</surname><given-names>DP</given-names></name><name><surname>Hotson</surname><given-names>G</given-names></name><name><surname>Katyal</surname><given-names>KD</given-names></name><name><surname>Wester</surname><given-names>BA</given-names></name><name><surname>Fifer</surname><given-names>MS</given-names></name><name><surname>McGee</surname><given-names>TG</given-names></name><name><surname>Harris</surname><given-names>A</given-names></name><name><surname>Johannes</surname><given-names>MS</given-names></name><name><surname>Vogelstein</surname><given-names>RJ</given-names></name><name><surname>Ravitz</surname><given-names>AD</given-names></name><name><surname>Anderson</surname><given-names>WS</given-names></name><name><surname>Thakor</surname><given-names>NV</given-names></name><name><surname>Crone</surname><given-names>NE</given-names></name></person-group><article-title>Demonstration of a semi-autonomous hybrid brain-machine interface using human intracranial EEG, eye tracking, and computer vision to control a robotic upper limb prosthetic</article-title><source>IEEE Trans Neural Syst Rehabil Eng</source><year>2014</year><volume>22</volume><fpage>784</fpage><lpage>796</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2013.2294685</pub-id><pub-id pub-id-type="pmid">24760914</pub-id></element-citation></ref></ref-list></back></article>