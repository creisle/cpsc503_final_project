% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Plaven-Sigray2017-xb,
  title    = "The readability of scientific texts is decreasing over time",
  author   = "Plav{\'e}n-Sigray, Pontus and Matheson, Granville James and
              Schiffler, Bj{\"o}rn Christian and Thompson, William Hedley",
  abstract = "Clarity and accuracy of reporting are fundamental to the
              scientific process. Readability formulas can estimate how
              difficult a text is to read. Here, in a corpus consisting of
              709,577 abstracts published between 1881 and 2015 from 123
              scientific journals, we show that the readability of science is
              steadily decreasing. Our analyses show that this trend is
              indicative of a growing use of general scientific jargon. These
              results are concerning for scientists and for the wider public,
              as they impact both the reproducibility and accessibility of
              research findings.",
  journal  = "Elife",
  volume   =  6,
  month    =  sep,
  year     =  2017,
  keywords = "data analysis; jargon; metascience; readability; scientific
              communication",
  language = "en"
}

@ARTICLE{Leroy2014-ft,
  title    = "The effect of word familiarity on actual and perceived text
              difficulty",
  author   = "Leroy, Gondy and Kauchak, David",
  abstract = "There is little evidence that readability formula outcomes relate
              to text understanding. The potential cause may lie in their
              strong reliance on word and sentence length. We evaluated word
              familiarity rather than word length as a stand-in for word
              difficulty. Word familiarity represents how well known a word is,
              and is estimated using word frequency in a large text corpus, in
              this work the Google web corpus. We conducted a study with 239
              people, who provided 50 evaluations for each of 275 words. Our
              study is the first study to focus on actual difficulty, measured
              with a multiple-choice task, in addition to perceived difficulty,
              measured with a Likert scale. Actual difficulty was correlated
              with word familiarity (r=0.219, p<0.001) but not with word length
              (r=-0.075, p=0.107). Perceived difficulty was correlated with
              both word familiarity (r=-0.397, p<0.001) and word length
              (r=0.254, p<0.001).",
  journal  = "J. Am. Med. Inform. Assoc.",
  volume   =  21,
  number   = "e1",
  pages    = "e169--72",
  month    =  feb,
  year     =  2014,
  keywords = "Comprehension; Health Literacy; Readability; Text Simplification;
              User Study",
  language = "en"
}

@ARTICLE{Beltagy2019-vz,
  title         = "{SciBERT}: A Pretrained Language Model for Scientific Text",
  author        = "Beltagy, Iz and Lo, Kyle and Cohan, Arman",
  abstract      = "Obtaining large-scale annotated data for NLP tasks in the
                   scientific domain is challenging and expensive. We release
                   SciBERT, a pretrained language model based on BERT (Devlin
                   et al., 2018) to address the lack of high-quality,
                   large-scale labeled scientific data. SciBERT leverages
                   unsupervised pretraining on a large multi-domain corpus of
                   scientific publications to improve performance on downstream
                   scientific NLP tasks. We evaluate on a suite of tasks
                   including sequence tagging, sentence classification and
                   dependency parsing, with datasets from a variety of
                   scientific domains. We demonstrate statistically significant
                   improvements over BERT and achieve new state-of-the-art
                   results on several of these tasks. The code and pretrained
                   models are available at https://github.com/allenai/scibert/.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1903.10676"
}

@ARTICLE{Augenstein2017-pg,
  title         = "{SemEval} 2017 Task 10: {ScienceIE} - Extracting Keyphrases
                   and Relations from Scientific Publications",
  author        = "Augenstein, Isabelle and Das, Mrinal and Riedel, Sebastian
                   and Vikraman, Lakshmi and McCallum, Andrew",
  abstract      = "We describe the SemEval task of extracting keyphrases and
                   relations between them from scientific documents, which is
                   crucial for understanding which publications describe which
                   processes, tasks and materials. Although this was a new
                   task, we had a total of 26 submissions across 3 evaluation
                   scenarios. We expect the task and the findings reported in
                   this paper to be relevant for researchers working on
                   understanding scientific content, as well as the broader
                   knowledge base population and information extraction
                   communities.",
  month         =  apr,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1704.02853"
}

@ARTICLE{Graesser2004-lg,
  title    = "Coh-metrix: analysis of text on cohesion and language",
  author   = "Graesser, Arthur C and McNamara, Danielle S and Louwerse, Max M
              and Cai, Zhiqiang",
  abstract = "Advances in computational linguistics and discourse processing
              have made it possible to automate many language- and
              text-processing mechanisms. We have developed a computer tool
              called Coh-Metrix, which analyzes texts on over 200 measures of
              cohesion, language, and readability. Its modules use lexicons,
              part-of-speech classifiers, syntactic parsers, templates,
              corpora, latent semantic analysis, and other components that are
              widely used in computational linguistics. After the user enters
              an English text, CohMetrix returns measures requested by the
              user. In addition, a facility allows the user to store the
              results of these analyses in data files (such as Text, Excel, and
              SPSS). Standard text readability formulas scale texts on
              difficulty by relying on word length and sentence length, whereas
              Coh-Metrix is sensitive to cohesion relations, world knowledge,
              and language and discourse characteristics.",
  journal  = "Behav. Res. Methods Instrum. Comput.",
  volume   =  36,
  number   =  2,
  pages    = "193--202",
  month    =  may,
  year     =  2004,
  language = "en"
}

@ARTICLE{Kauchak2017-ox,
  title    = "Measuring Text Difficulty Using {Parse-Tree} Frequency",
  author   = "Kauchak, David and Leroy, Gondy and Hogue, Alan",
  abstract = "Text simplification often relies on dated, unproven readability
              formulas. As an alternative and motivated by the success of term
              familiarity, we test a complementary measure: grammar
              familiarity. Grammar familiarity is measured as the frequency of
              the 3rd level sentence parse tree and is useful for evaluating
              individual sentences. We created a database of 140K unique 3rd
              level parse structures by parsing and binning all 5.4M sentences
              in English Wikipedia. We then calculated the grammar frequencies
              across the corpus and created 11 frequency bins. We evaluate the
              measure with a user study and corpus analysis. For the user
              study, we selected 20 sentences randomly from each bin,
              controlling for sentence length and term frequency, and recruited
              30 readers per sentence (N=6,600) on Amazon Mechanical Turk. We
              measured actual difficulty (comprehension) using a Cloze test,
              perceived difficulty using a 5-point Likert scale, and time
              taken. Sentences with more frequent grammatical structures, even
              with very different surface presentations, were easier to
              understand, perceived as easier and took less time to read.
              Outcomes from readability formulas correlated with perceived but
              not with actual difficulty. Our corpus analysis shows how the
              metric can be used to understand grammar regularity in a broad
              range of corpora.",
  journal  = "J Assoc Inf Sci Technol",
  volume   =  68,
  number   =  9,
  pages    = "2088--2100",
  month    =  sep,
  year     =  2017,
  keywords = "Comprehension; Health Literacy; Patient Education; Readability;
              Text Difficulty; Text Simplification",
  language = "en"
}

@ARTICLE{Balyan2020-zr,
  title     = "Applying natural language processing and hierarchical machine
               learning approaches to text difficulty classification",
  author    = "Balyan, Renu and McCarthy, Kathryn S and McNamara, Danielle S",
  journal   = "International Journal of Artificial Intelligence in Education",
  publisher = "Springer",
  volume    =  30,
  number    =  3,
  pages     = "337--370",
  year      =  2020
}

@ARTICLE{Guz2020-ap,
  title         = "Neural {RST-based} Evaluation of Discourse Coherence",
  author        = "Guz, Grigorii and Bateni, Peyman and Muglich, Darius and
                   Carenini, Giuseppe",
  abstract      = "This paper evaluates the utility of Rhetorical Structure
                   Theory (RST) trees and relations in discourse coherence
                   evaluation. We show that incorporating silver-standard RST
                   features can increase accuracy when classifying coherence.
                   We demonstrate this through our tree-recursive neural model,
                   namely RST-Recursive, which takes advantage of the text's
                   RST features produced by a state of the art RST parser. We
                   evaluate our approach on the Grammarly Corpus for Discourse
                   Coherence (GCDC) and show that when ensembled with the
                   current state of the art, we can achieve the new state of
                   the art accuracy on this benchmark. Furthermore, when
                   deployed alone, RST-Recursive achieves competitive accuracy
                   while having 62\% fewer parameters.",
  month         =  sep,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2009.14463"
}

@ARTICLE{Crossley2019-ln,
  title    = "The Tool for the Automatic Analysis of Cohesion 2.0: Integrating
              semantic similarity and text overlap",
  author   = "Crossley, Scott A and Kyle, Kristopher and Dascalu, Mihai",
  abstract = "This article introduces the second version of the Tool for the
              Automatic Analysis of Cohesion (TAACO 2.0). Like its predecessor,
              TAACO 2.0 is a freely available text analysis tool that works on
              the Windows, Mac, and Linux operating systems; is housed on a
              user's hard drive; is easy to use; and allows for batch
              processing of text files. TAACO 2.0 includes all the original
              indices reported for TAACO 1.0, but it adds a number of new
              indices related to local and global cohesion at the semantic
              level, reported by latent semantic analysis, latent Dirichlet
              allocation, and word2vec. The tool also includes a source overlap
              feature, which calculates lexical and semantic overlap between a
              source and a response text (i.e., cohesion between the two texts
              based measures of text relatedness). In the first study in this
              article, we examined the effects that cohesion features, prompt,
              essay elaboration, and enhanced cohesion had on expert ratings of
              text coherence, finding that global semantic similarity as
              reported by word2vec was an important predictor of coherence
              ratings. A second study was conducted to examine the source and
              response indices. In this study we examined whether source
              overlap between the speaking samples found in the TOEFL-iBT
              integrated speaking tasks and the responses produced by
              test-takers was predictive of human ratings of speaking
              proficiency. The results indicated that the percentage of
              keywords found in both the source and response and the similarity
              between the source document and the response, as reported by
              word2vec, were significant predictors of speaking quality.
              Combined, these findings help validate the new indices reported
              for TAACO 2.0.",
  journal  = "Behav. Res. Methods",
  volume   =  51,
  number   =  1,
  pages    = "14--27",
  month    =  feb,
  year     =  2019,
  keywords = "Coherence; Cohesion; Essay quality; Natural language processing;
              Speaking proficiency",
  language = "en"
}

@ARTICLE{Lai2018-qp,
  title         = "Discourse Coherence in the Wild: A Dataset, Evaluation and
                   Methods",
  author        = "Lai, Alice and Tetreault, Joel",
  abstract      = "To date there has been very little work on assessing
                   discourse coherence methods on real-world data. To address
                   this, we present a new corpus of real-world texts (GCDC) as
                   well as the first large-scale evaluation of leading
                   discourse coherence algorithms. We show that neural models,
                   including two that we introduce here (SentAvg and ParSeq),
                   tend to perform best. We analyze these performance
                   differences and discuss patterns we observed in low
                   coherence texts in four domains.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1805.04993"
}

@INPROCEEDINGS{Brack2020-id,
  title     = "{Domain-Independent} Extraction of Scientific Concepts from
               Research Articles",
  booktitle = "Advances in Information Retrieval",
  author    = "Brack, Arthur and D'Souza, Jennifer and Hoppe, Anett and Auer,
               S{\"o}ren and Ewerth, Ralph",
  abstract  = "We examine the novel task of domain-independent scientific
               concept extraction from abstracts of scholarly articles and
               present two contributions. First, we suggest a set of generic
               scientific concepts that have been identified in a systematic
               annotation process. This set of concepts is utilised to annotate
               a corpus of scientific abstracts from 10 domains of Science,
               Technology and Medicine at the phrasal level in a joint effort
               with domain experts. The resulting dataset is used in a set of
               benchmark experiments to (a) provide baseline performance for
               this task, (b) examine the transferability of concepts between
               domains. Second, we present a state-of-the-art deep learning
               baseline. Further, we propose the active learning strategy for
               an optimal selection of instances from among the various domains
               in our data. The experimental results show that (1) a
               substantial agreement is achievable by non-experts after
               consultation with domain experts, (2) the baseline system
               achieves a fairly high F1 score, (3) active learning enables us
               to nearly halve the amount of required training data.",
  publisher = "Springer International Publishing",
  pages     = "251--266",
  year      =  2020
}

@INPROCEEDINGS{Krishnan2017-eo,
  title     = "Unsupervised Concept Categorization and Extraction from
               Scientific Document Titles",
  booktitle = "Proceedings of the 2017 {ACM} on Conference on Information and
               Knowledge Management",
  author    = "Krishnan, Adit and Sankar, Aravind and Zhi, Shi and Han, Jiawei",
  abstract  = "This paper studies the automated categorization and extraction
               of scientific concepts from titles of scientific articles, in
               order to gain a deeper understanding of their key contributions
               and facilitate the construction of a generic academic
               knowledgebase. Towards this goal, we propose an unsupervised,
               domain-independent, and scalable two-phase algorithm to type and
               extract key concept mentions into aspects of interest (e.g.,
               Techniques, Applications, etc.). In the first phase of our
               algorithm we proposePhraseType, a probabilistic generative model
               which exploits textual features and limited POS tags to broadly
               segment text snippets into aspect-typed phrases. We extend this
               model to simultaneously learn aspect-specific features and
               identify academic domains in multi-domain corpora, since the two
               tasks mutually enhance each other. In the second phase, we
               propose an approach based on adaptor grammars to extract fine
               grained concept mentions from the aspect-typed phrases without
               the need for any external resources or human effort, in a purely
               data-driven manner. We apply our technique to study literature
               from diverse scientific domains and show significant gains over
               state-of-the-art concept extraction techniques. We also present
               a qualitative analysis of the results obtained.",
  publisher = "Association for Computing Machinery",
  pages     = "1339--1348",
  series    = "CIKM '17",
  month     =  nov,
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "adaptor grammar, concept extraction, probabilistic model",
  location  = "Singapore, Singapore"
}

@ARTICLE{Xu2020-hq,
  title    = "Building a {PubMed} knowledge graph",
  author   = "Xu, Jian and Kim, Sunkyu and Song, Min and Jeong, Minbyul and
              Kim, Donghyeon and Kang, Jaewoo and Rousseau, Justin F and Li,
              Xin and Xu, Weijia and Torvik, Vetle I and Bu, Yi and Chen,
              Chongyan and Ebeid, Islam Akef and Li, Daifeng and Ding, Ying",
  abstract = "PubMed\textregistered{} is an essential resource for the medical
              domain, but useful concepts are either difficult to extract or
              are ambiguous, which has significantly hindered knowledge
              discovery. To address this issue, we constructed a PubMed
              knowledge graph (PKG) by extracting bio-entities from 29 million
              PubMed abstracts, disambiguating author names, integrating
              funding data through the National Institutes of Health (NIH)
              ExPORTER, collecting affiliation history and educational
              background of authors from ORCID\textregistered{}, and
              identifying fine-grained affiliation data from MapAffil. Through
              the integration of these credible multi-source data, we could
              create connections among the bio-entities, authors, articles,
              affiliations, and funding. Data validation revealed that the
              BioBERT deep learning method of bio-entity extraction
              significantly outperformed the state-of-the-art models based on
              the F1 score (by 0.51\%), with the author name disambiguation
              (AND) achieving an F1 score of 98.09\%. PKG can trigger broader
              innovations, not only enabling us to measure scholarly impact,
              knowledge usage, and knowledge transfer, but also assisting us in
              profiling authors and organizations based on their connections
              with bio-entities.",
  journal  = "Sci Data",
  volume   =  7,
  number   =  1,
  pages    = "205",
  month    =  jun,
  year     =  2020,
  language = "en"
}


@BOOK{Sayers2010-zl,
  title     = "A General Introduction to the E-utilities",
  author    = "Sayers, Eric",
  abstract  = "The Entrez Programming Utilities (E-utilities) are a set of nine
               server-side programs that provide a stable interface into the
               Entrez query and database system at the National Center for
               Biotechnology Information (NCBI). The E-utilities use a fixed
               URL syntax that translates a standard set of input parameters
               into the values necessary for various NCBI software components
               to search for and retrieve the requested data. The E-utilities
               are therefore the structured interface to the Entrez system,
               which currently includes 38 databases covering a variety of
               biomedical data, including nucleotide and protein sequences,
               gene records, three-dimensional molecular structures, and the
               biomedical literature.",
  publisher = "National Center for Biotechnology Information (US)",
  year      =  2010
}
