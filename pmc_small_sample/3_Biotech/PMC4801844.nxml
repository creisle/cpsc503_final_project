<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">3 Biotech</journal-id><journal-id journal-id-type="iso-abbrev">3 Biotech</journal-id><journal-title-group><journal-title>3 Biotech</journal-title></journal-title-group><issn pub-type="ppub">2190-572X</issn><issn pub-type="epub">2190-5738</issn><publisher><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28330163</article-id><article-id pub-id-type="pmc">4801844</article-id><article-id pub-id-type="publisher-id">410</article-id><article-id pub-id-type="doi">10.1007/s13205-016-0410-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>Probing an optimal class distribution for enhancing prediction and feature characterization of plant virus-encoded RNA-silencing suppressors</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Nath</surname><given-names>Abhigyan</given-names></name><address><phone>+91-9956015187</phone><email>abhigyannath01@gmail.com</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Subbiah</surname><given-names>Karthikeyan</given-names></name><address><phone>+91-9473967721</phone><email>karthinikita@gmail.com</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1">Department of Computer Science, Banaras Hindu University, Varanasi, India </aff></contrib-group><pub-date pub-type="epub"><day>21</day><month>3</month><year>2016</year></pub-date><pub-date pub-type="pmc-release"><day>21</day><month>3</month><year>2016</year></pub-date><pub-date pub-type="ppub"><month>6</month><year>2016</year></pub-date><volume>6</volume><issue>1</issue><elocation-id>93</elocation-id><history><date date-type="received"><day>30</day><month>9</month><year>2015</year></date><date date-type="accepted"><day>3</day><month>3</month><year>2016</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2016</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p>To counter
the host RNA silencing defense mechanism, many plant viruses encode RNA silencing suppressor proteins. These groups of proteins share very low sequence and structural similarities among them, which consequently hamper their annotation using sequence similarity-based search methods. Alternatively the machine learning-based methods can become a suitable choice, but the optimal performance through machine learning-based methods is being affected by various factors such as class imbalance, incomplete learning, selection of inappropriate features, etc. In this paper, we have proposed a novel approach to deal with the class imbalance problem by finding the optimal class distribution for enhancing the prediction accuracy for the RNA silencing suppressors. The optimal class distribution was obtained using different resampling techniques with varying degrees of class distribution starting from natural distribution to ideal distribution, i.e., equal distribution. The experimental results support the fact that optimal class distribution plays an important role to achieve near perfect learning. The best prediction results are obtained with Sequential Minimal Optimization (SMO) learning algorithm. We could achieve a sensitivity of 98.5&#x000a0;%, specificity of 92.6&#x000a0;% with an overall accuracy of 95.3&#x000a0;% on a tenfold cross validation and is further validated using leave one out cross validation test. It was also observed that the machine learning models trained on oversampled training sets using synthetic minority oversampling technique (SMOTE) have relatively performed better than on both randomly undersampled and imbalanced training data sets. Further, we have characterized the important discriminatory sequence features of RNA-silencing suppressors which distinguish these groups of proteins from other protein families.</p><sec><title>Electronic supplementary material</title><p>The online version of this article (doi:10.1007/s13205-016-0410-1) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>RNA silencing</kwd><kwd>Class imbalance problem</kwd><kwd>Optimal class distribution</kwd><kwd>Balanced training set</kwd><kwd>SMOTE</kwd><kwd>Random undersampling</kwd><kwd>SVM</kwd><kwd>Relieff</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; King Abdulaziz City for Science and Technology 2016</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p>RNA silencing is a common host defense mechanism in plants against many plant RNA/DNA viruses (Li et al. <xref ref-type="bibr" rid="CR27">2014a</xref>; P&#x000e9;rez-Ca&#x000f1;am&#x000e1;s and Hern&#x000e1;ndez <xref ref-type="bibr" rid="CR38">2014</xref>; Valli et al. <xref ref-type="bibr" rid="CR44">2001</xref>). To counter the RNA silencing defense mechanism, these plant viruses encode RNA-silencing suppressors, which disturb the host RNA silencing pathway. The molecular basis for the mechanism of encoding RNA-silencing suppressors by these plant viruses is still largely unknown. P1/HC&#x02013;Pro of Potyviruses, P19 of tombusviruses and 2b proteins of cucumo-viruses are some of the well-studied RNA silencing suppressors (Qu and Morris <xref ref-type="bibr" rid="CR41">2005</xref>) and recently new RNA silencing suppressors are being identified in a mastrevirus (Wang et al. <xref ref-type="bibr" rid="CR47">2014</xref>) and in a wheat dwarf virus (Liu et al. <xref ref-type="bibr" rid="CR29">2014</xref>). Recent studies have also pointed to the role of suppressors in modulating the function of microRNAs (Chapman et al. <xref ref-type="bibr" rid="CR9">2004</xref>; Dunoyer et al. <xref ref-type="bibr" rid="CR14">2004</xref>).</p><p>Annotation of putative members of this family is hampered by the presence of high sequence diversity existing among these plant virus-encoded RNA-silencing suppressors (Qu and Morris <xref ref-type="bibr" rid="CR41">2005</xref>). The sequence similarity-based search methods like BLAST (Altschul et al. <xref ref-type="bibr" rid="CR1">1990</xref>) and PSI-BLAST (Altschul et al. <xref ref-type="bibr" rid="CR2">1997</xref>) have their inherent limitations in these situations where there exists low sequence conservation. Previously in (Jagga and Gupta <xref ref-type="bibr" rid="CR20">2014</xref>) the shortcomings of sequence similarity-based search methods like PSI-BLAST in correctly annotating the members of this protein family are emphasized. Machine learning methods trained on mathematically represented suitable input feature vectors become a viable alternative to sequence similarity-based search methods. Previously different machine learning methods have been successfully applied to solve biological classification tasks (Kumari et al. <xref ref-type="bibr" rid="CR24">2015</xref>; Nath et al. <xref ref-type="bibr" rid="CR36">2012</xref>; Nath and Subbiah <xref ref-type="bibr" rid="CR33">2014</xref>). But the true performance of machine learning methods is affected by various factors such as class imbalance (Nath and Subbiah <xref ref-type="bibr" rid="CR34">2015a</xref>), imperfect learning due to some missing example instances and selection of inappropriate input features.</p><p>The class imbalance problem is quite common in biological datasets, where there is a huge difference in the number of instances belonging to the different classes and subclasses. These types of imbalanced datasets result in classifier bias towards the majority class and tend to produce majority class classifier (Wei and Dunbrack <xref ref-type="bibr" rid="CR48">2013</xref>). In most of the cases, the class of interest is the minority class and is the cause for lower sensitivity. Many methods had been proposed to deal with the class imbalance problem. Previously it has been stressed that the natural class distribution may not be optimal for training (Lee <xref ref-type="bibr" rid="CR25">2014</xref>; Weiss and Provost <xref ref-type="bibr" rid="CR49">2003</xref>) and the requirement of a balanced training set for proper learning has been pointed out by Dunbrack et al. (Wei and Dunbrack <xref ref-type="bibr" rid="CR48">2013</xref>). In the current work, we propose a technique to achieve better learning of both the positive and negative classes by experimenting with different resampling methods to balance the dataset with varying degree of class distributions. We have also repeated the experiments on different machine learning algorithms on imbalanced, Synthetic Minority Oversampling Technique (SMOTE) (Chawla et al. <xref ref-type="bibr" rid="CR10">2002</xref>) oversampled and randomly undersampled datasets to find the optimal class distribution. We used the sequence features like amino acid composition, property group composition, dipeptide counts and property group n-grams for creating the input feature vectors. Broadly, two types of approaches are used for handling the class imbalance, (1) resampling methods which are algorithm independent and are transferable to different machine learning algorithms and (2) internal approaches which involve altering the existing algorithms and its various parameters for adapting to imbalance class distribution. The SMOTE and random undersampling fall under resampling methods, although other sophisticated varieties of SMOTE exist (Barua et al. <xref ref-type="bibr" rid="CR4">2014</xref>; Han et al. <xref ref-type="bibr" rid="CR19">2005</xref>; Nakamura et al. <xref ref-type="bibr" rid="CR32">2013</xref>), but in the present study, we have limited our focus on simple undersampling and SMOTE oversampling as they are found to be useful for many classifiers (Blagus and Lusa <xref ref-type="bibr" rid="CR6">2013</xref>) and in many biological classification problems (Batuwita and Palade <xref ref-type="bibr" rid="CR5">2009</xref>; MacIsaac et al. <xref ref-type="bibr" rid="CR30">2006</xref>; Xiao et al. <xref ref-type="bibr" rid="CR50">2011</xref>).</p><p>The current method explored the possibility of improvement in prediction accuracy of the machine learning algorithms using optimal class distribution and presented in detail the behavior of the tested learning algorithms with varying degrees of resampling. From the current work, it is also proved that prediction accuracy for the plant virus-encoded RNA-silencing suppressor proteins can be improved using resampling techniques.</p></sec><sec id="Sec2"><title>Materials and methods</title><sec id="Sec3"><title>Dataset</title><p>We have used the dataset as used in (Jagga and Gupta <xref ref-type="bibr" rid="CR20">2014</xref>) which consisted of 208 plant virus-encoded RNA-silencing suppressor proteins (RSSPs) belonging to positive class and 1321 non-suppressor proteins (NSPs) belonging to negative class, for this study. The CD-HIT (Li and Godzik <xref ref-type="bibr" rid="CR26">2006</xref>) was applied separately to these classes of sequences to reduce the redundancy at 70&#x000a0;% sequence identity. Here, the positive class is the minority class as the number of positive class sequences is relatively very small when compared to the number of negative class sequences and their prediction will suffer from the imbalance class factor.</p></sec><sec id="Sec4"><title>Extraction of feature vectors</title><p>The quality of the attributes of the protein sequences selected for creating the input feature vector will have great influence in learning the concepts of a particular protein family. We represented each protein sequence as the combination of following sequence features to create input instances and they are explained below.</p><sec id="Sec5"><title>Amino acid composition feature</title><p>Different proteins are evolved through the avoidance and preference of some specific amino acids and leads to some certain unique set of percentage frequency composition, which can be used successfully for discriminatory purposes (Nath and Subbiah <xref ref-type="bibr" rid="CR33">2014</xref>). So we have taken the frequency percentage of distribution of the 20 amino acids along the length of the protein sequence as one of the features for creating the input feature vector. It is calculated using the following formula:</p><p>
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{AA}}_{i} = \frac{{{\text{TC}}_{{{\text{AA}},i}} }}{{{\text{TC}}_{{{\text{res}}, i}} }} \times 100, $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:msub><mml:mtext>AA</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mtext>TC</mml:mtext><mml:mrow><mml:mtext>AA</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mtext>TC</mml:mtext><mml:mrow><mml:mtext>res</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where AA denotes for one of the 20 amino acid residues, AA<sub><italic>i</italic></sub> denotes the amino acid percentage frequency of specific type &#x02018;AA&#x02019; in the <italic>i</italic>th Sequence, TC<sub>AA,<italic>i</italic></sub> denotes the total count of amino acid of specific &#x02018;AA&#x02019; type in the <italic>i</italic>th sequence, TC<sub>res,<italic>i</italic></sub> denotes the total count of all residues in the <italic>i</italic>th sequence (i.e., sequence length).</p></sec><sec id="Sec6"><title>Amino acid property group composition feature</title><p>The amino acids can be grouped according to their physicochemical properties. The Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> contains the list of amino acids belonging to the 11 different physicochemical groups. We have taken the percentage frequency composition of the 11 different amino acid property groups as used in (Nath et al. <xref ref-type="bibr" rid="CR37">2013</xref>) as the second feature. The formula for calculating this feature attribute is given below.<table-wrap id="Tab1"><label>Table&#x000a0;1</label><caption><p>Physicochemical groupings of amino acids taken for the present study</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">S. no.</th><th align="left">Name of amino acid property group</th><th align="left">Amino acids in the specific group</th></tr></thead><tbody><tr><td align="left">1.</td><td align="left">Tiny amino acids group</td><td align="left">Ala, Cys, Gly, Ser, Thr</td></tr><tr><td align="left">2.</td><td align="left">Small amino acids group</td><td align="left">Ala, Cys, Asp, Gly, Asn, Pro, Ser, Thr and Val</td></tr><tr><td align="left">3.</td><td align="left">Aliphatic amino acids group</td><td align="left">Ile, Leu and Val</td></tr><tr><td align="left">4.</td><td align="left">Nonpolar amino acid groups</td><td align="left">Ala, Cys, Phe, Gly, Ile, Leu, Met, Pro, Val, Trp and Tyr</td></tr><tr><td align="left">5.</td><td align="left">Aromatic amino acid group</td><td align="left">Phe, His, Trp and Tyr</td></tr><tr><td align="left">6.</td><td align="left">Polar amino acid group</td><td align="left">Asp, Glu, His, Lys, Asn, Gln. Arg, Ser, and Thr</td></tr><tr><td align="left">7.</td><td align="left">Charged amino acid group</td><td align="left">Asp, Glu, His, Arg, Lys</td></tr><tr><td align="left">8.</td><td align="left">Basic amino acid group</td><td align="left">His, Lys and Arg</td></tr><tr><td align="left">9.</td><td align="left">Acidic amino acid group</td><td align="left">Asp and Glu</td></tr><tr><td align="left">10.</td><td align="left">Hydrophobic acid group</td><td align="left">Ala, Cys, Phe, Ile, Leu, Met, Val, Trp, Tyr</td></tr><tr><td align="left">11.</td><td align="left">Hydrophilic acid group</td><td align="left">Asp, Glu, Lys, Asn, Gln</td></tr></tbody></table></table-wrap>
</p><p>
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{PG}}_{i} = \frac{{{\text{TC}}_{{{\text{PG}}, i}} }}{{{\text{TC}}_{{{\text{res}}, i}} }} \times  1 0 0 , $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mtext>PG</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mtext>TC</mml:mtext><mml:mrow><mml:mtext>PG</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mtext>TC</mml:mtext><mml:mrow><mml:mtext>res</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where PG denotes one of the 11 different amino acid property groups, PG<sub><italic>i</italic></sub> denotes the percentage frequency of specific &#x02018;PG&#x02019; amino acid property group in the <italic>i</italic>th sequence, TC<sub>PG,<italic>i</italic></sub> denotes the total count of specific amino acid property group &#x02018;PG&#x02019; in the <italic>i</italic>th sequence, TC<sub>res,<italic>i</italic></sub> denotes the total count of all residues in the <italic>i</italic>th sequence.</p></sec><sec id="Sec7"><title>Dipeptide counts</title><p>There are four hundred different possible dipeptides from 20 amino acids. To take advantage of the local sequence order and amino acid coupling into the prediction we have taken the dipeptide counts as the third feature.</p></sec><sec id="Sec8"><title>Property group <italic>n</italic>-grams</title><p>To take into the conservation of similar contiguous physicochemical amino acid property groups in the protein sequence, we have calculated the property groups <italic>n</italic>-grams, where n is the window length. In the current work we have taken the window length of 2 as the fourth feature and is calculated by the formula given below:</p><p>
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Physicochemical}}\,\; 2 {\text{-grams}}:\,\;{\text{Small}} = \sum\limits_{i = 1}^{N - 1} {C\left({i,i + 1} \right)}, $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mtext>Physicochemical</mml:mtext><mml:mspace width="0.166667em"/><mml:mspace width="0.277778em"/><mml:mn>2</mml:mn><mml:mtext>-grams</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="0.166667em"/><mml:mspace width="0.277778em"/><mml:mtext>Small</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>C</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>N</italic> denotes the length of the protein sequence, <italic>i</italic> denotes the position of the amino acid residue along the protein sequence, if the condition <inline-formula id="IEq1"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ ({\text{aa}}_{i} \in S^{*}   {\text{and aa}}_{i + 1} \in S^{*} ) $$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>aa</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mrow/><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mtext>and aa</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mrow/><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13205_2016_410_Article_IEq1.gif"/></alternatives></inline-formula> is true then <inline-formula id="IEq2"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ C(i,i + 1) $$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13205_2016_410_Article_IEq2.gif"/></alternatives></inline-formula>&#x000a0;=&#x000a0;1 else <inline-formula id="IEq3"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ C(i,i + 1) $$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13205_2016_410_Article_IEq3.gif"/></alternatives></inline-formula>&#x000a0;=&#x000a0;0 where the set of small aminoacids S<sup>*</sup>&#x000a0;=&#x000a0;{Ala,Cys,Asp,Gly,Asn,Pro,Ser,Thr,Val}.</p><p>The above formula is used to calculate physicochemical 2-grams for the small amino acid group. In the similar way the physicochemical 2-grams for the other ten physicochemical property groups were calculated. An example feature vector is provided in Supplementary Table S1&#x02013;S3.</p></sec></sec><sec id="Sec9"><title>Optimal balancing protocol</title><sec id="Sec10"><title>SMOTE</title><p>It was proposed by Chawla et al. (<xref ref-type="bibr" rid="CR10">2002</xref>) for intelligent oversampling of minority samples as opposed to random oversampling, which may bias the learning towards the overrepresented samples. It is a nearest neighbor-based method, where it first chooses k nearest samples for a particular minority sample. It then randomly selects the j minority samples to create a synthetic minority sample. Successful use of SMOTE in classification tasks have been shown in (Li et al. <xref ref-type="bibr" rid="CR28">2014b</xref>; Nath and Subbiah <xref ref-type="bibr" rid="CR35">2015b</xref>; Suvarna Vani and Durga Bhavani <xref ref-type="bibr" rid="CR43">2013</xref>).</p></sec><sec id="Sec11"><title>Classification protocol SVM</title><p>Support vector machines are supervised learning algorithms and are based on statistical learning theory of Vapnik (Vapnik <xref ref-type="bibr" rid="CR45">1995</xref>, <xref ref-type="bibr" rid="CR46">1998</xref>). Previous usage of SVM for biological classification/prediction problems has found them to be more accurate and also they are robust to noise and well suited for high dimensional datasets (Kandaswamy et al. <xref ref-type="bibr" rid="CR21">2011</xref>; Mishra et al. <xref ref-type="bibr" rid="CR31">2014</xref>; Pugalenthi et al. <xref ref-type="bibr" rid="CR40">2010</xref>). We have used the sequential minimization optimization (SMO) (Platt <xref ref-type="bibr" rid="CR39">1999</xref>) algorithm for fast training of SVM with polynomial kernel with an exponent value of 1 and <italic>C</italic>&#x000a0;=&#x000a0;1 (a complexity parameter which SMO uses to build the hyperplane between the two classes, -C governs softness of the class margins).</p><p>All the experiments were carried out using WEKA (Hall et al. <xref ref-type="bibr" rid="CR18">2009</xref>) which is an open source java-based machine learning platform. The schematic representation of the current methodology is given in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>Schematic representation of the current pipeline</p></caption><graphic xlink:href="13205_2016_410_Fig1_HTML" id="MO4"/></fig>
</p></sec></sec><sec id="Sec12"><title>Characterization of plant virus-encoded RNA-silencing suppressors</title><p>We have used Relieff (Kira and Rendell <xref ref-type="bibr" rid="CR22">1992</xref>) feature ranking algorithm to rank the sequence features according to their discriminating ability. Relieff is a nearest neighbor-based feature relevance algorithm. It starts by randomly selecting an instance and then searches for the nearest neighboring instances belonging to the same and opposite classes. It compares the attributes of the instance with its nearest neighbors and assigns weights according to its discriminating ability.</p></sec><sec id="Sec13"><title>Performance evaluation metrics</title><p>We have used stratified tenfold cross validation for the evaluation of the various models. The performances of the machine learning algorithms were assessed with both threshold-dependent and threshold-independent parameters. These parameters are derived from the values of the confusion matrix, namely TP: true positive that is the number of correctly predicted RSSPs, TN: true negative that is the number of correctly predicted NSPs, FP: false positive that is the number of incorrectly predicted NSPs and FN: false negative that is the number of incorrectly predicted RSSPs. The formula for calculating the evaluation parameters are given below:</p><p>
<italic>Sensitivity</italic> Expresses the percentage of correctly predicted RSSPs.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Sensitivity}} = {{\text{TP}} \mathord{\left/ {\vphantom {{\text{TP}} {\left( {{\text{TP}} + {\text{FN}}} \right)}}} \right. \kern-0pt} {\left( {{\text{TP}} + {\text{FN}}} \right)}} \times 100. $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtext>Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mtext>TP</mml:mtext><mml:mrow><mml:mfenced close="" open="/" separators=""><mml:mrow/></mml:mfenced><mml:mspace width="0.0pt"/></mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>
</p><p>
<italic>Specificity</italic> Expresses the percentage of correctly predicted NSPs.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Specificity}} = {{\text{TN}} \mathord{\left/ {\vphantom {{\text{TN}} {\left( {{\text{TN}} + {\text{FP}}} \right)}}} \right. \kern-0pt} {\left( {{\text{TN}} + {\text{FP}}} \right)}} \times 100. $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtext>Specificity</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mtext>TN</mml:mtext><mml:mrow><mml:mfenced close="" open="/" separators=""><mml:mrow/></mml:mfenced><mml:mspace width="0.0pt"/></mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>
</p><p>
<italic>Accuracy</italic> Expresses the percentage of both correctly predicted RSSPs and NSPs.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Accuracy}} = {{\left( {{\text{TP}} + {\text{TN}}} \right)} \mathord{\left/ {\vphantom {{\left( {{\text{TP}} + {\text{TN}}} \right)} {\left( {{\text{TP}} + {\text{FP}} + {\text{TN}} + {\text{FN}}} \right)}}} \right. \kern-0pt} {\left( {{\text{TP}} + {\text{FP}} + {\text{TN}} + {\text{FN}}} \right)}} \times 100. $$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow></mml:mfenced><mml:mrow><mml:mfenced close="" open="/" separators=""><mml:mrow/></mml:mfenced><mml:mspace width="0.0pt"/></mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>
</p><p>
<italic>AUC</italic> Area under the receiver operating characteristic (ROC) curve that summarizes the ROC by a single numerical value. It is a threshold-independent metric and can take values from 0 to 1 (Bradley <xref ref-type="bibr" rid="CR7">1997</xref>). The value of 0 indicates the worst case, 0.5 for random ranking and 1 indicates the best prediction.</p><p>
<italic>Youden&#x02019;s Index</italic> This performance metric evaluates the algorithm&#x02019;s ability to avoid failure. Lower failure rates are expressed by higher index values (Youden <xref ref-type="bibr" rid="CR52">1950</xref>). It is calculated as:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Y = \left( {\text{Sensitivity}} \right) - \left( {1 - {\text{Specificity}}} \right). $$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mtext>Sensitivity</mml:mtext></mml:mfenced><mml:mo>-</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mtext>Specificity</mml:mtext></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>
</p><p>
<italic>Dominance</italic> It expresses the relationship between the TP_rate (true-positive rate) and TN_rate (true-negative rate) and is proposed by (Garc&#x000ed;a et al. <xref ref-type="bibr" rid="CR16">2009</xref>). It is calculated as:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Dominance}} = \left({\text{TP}} \_{\text{rate}}\right)-\left({\text{TN}} \_{\text{rate}}\right). $$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:mtext>Dominance</mml:mtext><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mtext>TP</mml:mtext><mml:mi>_</mml:mi><mml:mtext>rate</mml:mtext></mml:mfenced><mml:mo>-</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mtext>TN</mml:mtext><mml:mi>_</mml:mi><mml:mtext>rate</mml:mtext></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>
</p><p>Its value ranges from &#x02212;1 to +1. A dominance value of +1 means a perfect accuracy on the positive class and a value &#x02212;1 means a perfect accuracy on the negative class. A value closer to zero means a balance between TP_rate and TN_rate.</p><p>
<italic>g</italic>-<italic>mean</italic>: it was proposed by Kubat et al. (<xref ref-type="bibr" rid="CR23">1997</xref>), this evaluation parameter shows the balance between sensitivity and specificity. It is the geometric mean of sensitivity and specificity. It is calculated as:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ g{\text{-means}} = \sqrt {{\text{Sensitivity}} \times {\text{Specificity}}}. $$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mi>g</mml:mi><mml:mtext>-means</mml:mtext><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mtext>Sensitivity</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>Specificity</mml:mtext></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="13205_2016_410_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>
</p></sec></sec><sec id="Sec14"><title>Results and discussion</title><p>We have experimented with four different machine learning algorithms, namely&#x02014;(1) naive Bayes (NB), (2) Fischer linear discriminant function (implemented as FLDA in WEKA), (3) support vector machines with sequential minimization optimization (SMO) and (4) K nearest neighbor (implemented as IBK in WEKA) on the imbalanced dataset (original), randomly undersampled dataset (with varying class distribution) and SMOTE oversampled dataset (with varying class distribution) to find the optimal class distribution for each of these classifiers.</p><sec id="Sec15"><title>Learning performance on imbalanced dataset</title><p>Observing the values of the performance evaluation parameters obtained from the different machine learning algorithms when trained with the imbalanced dataset (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>), the overall accuracy of SMO and IBK crossed above 90&#x000a0;%, although with a large difference in their individual accuracies for the positive (sensitivity) and negative classes (specificity), respectively. The training on the imbalanced dataset resulted in high specificity values for all the learning algorithms except the naive Bayes. The negative dominance values of all the learning algorithms (except the naive Bayes) are also biased towards the TN_rate. This indicates that optimal learning with higher accuracies (sensitivity and specificities) for the positive and negative classes is difficult in cases where there is an imbalance between the positive and negative class instances.<table-wrap id="Tab2"><label>Table&#x000a0;2</label><caption><p>Performance evaluation metrics of the different learning algorithms trained on the imbalanced datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning algorithms</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">Accuracy</th><th align="left">AUC</th><th align="left">Youden&#x02019;s Index</th><th align="left">Dominance</th><th align="left">
<italic>g</italic>-means</th></tr></thead><tbody><tr><td align="left" colspan="8">Imbalanced data set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">90.8</td><td char="." align="char">29.2</td><td char="." align="char">36.9</td><td char="." align="char">0.678</td><td char="." align="char">0.200</td><td char="." align="char">0.616</td><td char="." align="char">51.49</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">64.7</td><td char="." align="char">84.9</td><td char="." align="char">82.3</td><td char="." align="char">0.819</td><td char="." align="char">0.492</td><td char="." align="char">&#x02212;0.202</td><td char="." align="char">74.1</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">52.1</td><td char="." align="char">97.1</td><td char="." align="char">91.4</td><td char="." align="char">0.746</td><td char="." align="char">0.496</td><td char="." align="char">&#x02212;0.450</td><td char="." align="char">71.1</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">68.9</td><td char="." align="char">97.0</td><td char="." align="char">93.4</td><td char="." align="char">0.841</td><td char="." align="char">0.659</td><td char="." align="char">&#x02212;0.281</td><td char="." align="char">81.7</td></tr></tbody></table></table-wrap>
</p></sec><sec id="Sec16"><title>Learning performance on randomly undersampled datasets</title><p>Nearest neighbor-based IBK method performed better than all the other machine learning algorithms and closely followed by SMO, when the original imbalance dataset was subjected to undersampling at different distribution rates for dealing with the data imbalance problem. The values of different performance evaluation parameters obtained by different degrees of class distribution are recorded in the Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. When the dataset is fully balanced by undersampling (undersampled 1:1), we obtained higher accuracy for the positive class samples than all other undersampled datasets. Highest overall accuracy of 91.8&#x000a0;% is obtained by IBK when the undersampling rate is 1:5 closely followed by SMO with 89.4&#x000a0;% accuracy. In the case of the undersampling datasets, IBK performed better than all other machine learning algorithms.<table-wrap id="Tab3"><label>Table&#x000a0;3</label><caption><p>Performance evaluation metrics of the different machine learning algorithms trained on the different randomly undersampled training sets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning algorithms</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">Accuracy</th><th align="left">AUC</th><th align="left">Youden&#x02019;s Index</th><th align="left">Dominance</th><th align="left">
<italic>g</italic>-means</th></tr></thead><tbody><tr><td align="left" colspan="8">Undersampling (1:1) (fully balanced) training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">91.6</td><td char="." align="char">23.5</td><td char="." align="char">57.6</td><td char="." align="char">0.631</td><td char="." align="char">0.151</td><td char="." align="char">0.681</td><td char="." align="char">46.3</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">73.9</td><td char="." align="char">68.5</td><td char="." align="char">71.4</td><td char="." align="char">0.768</td><td char="." align="char">0.424</td><td char="." align="char">0.054</td><td char="." align="char">71.1</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">77.3</td><td char="." align="char">74.8</td><td char="." align="char">76.1</td><td char="." align="char">0.761</td><td char="." align="char">0.521</td><td char="." align="char">0.025</td><td char="." align="char">76.0</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">80.7</td><td char="." align="char">81.5</td><td char="." align="char">81.1</td><td char="." align="char">0.818</td><td char="." align="char">0.622</td><td char="." align="char">&#x02212;0.008</td><td char="." align="char">81.5</td></tr><tr><td align="left" colspan="8">Undersampling (1:2) training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">89.1</td><td char="." align="char">30.3</td><td char="." align="char">49.9</td><td char="." align="char">0.666</td><td char="." align="char">0.194</td><td char="." align="char">0.588</td><td char="." align="char">51.9</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">63.0</td><td char="." align="char">63.0</td><td char="." align="char">63.0</td><td char="." align="char">0.661</td><td char="." align="char">0.26</td><td char="." align="char">0</td><td char="." align="char">63</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">72.3</td><td char="." align="char">88.7</td><td char="." align="char">83.2</td><td char="." align="char">0.805</td><td char="." align="char">0.61</td><td char="." align="char">&#x02212;0.164</td><td char="." align="char">80.08</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">72.3</td><td char="." align="char">90.8</td><td char="." align="char">84.6</td><td char="." align="char">0.809</td><td char="." align="char">0.631</td><td char="." align="char">&#x02212;0.185</td><td char="." align="char">81.0</td></tr><tr><td align="left" colspan="8">Undersampling (1:3) training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">90.8</td><td char="." align="char">28.9</td><td char="." align="char">44.3</td><td char="." align="char">0.664</td><td char="." align="char">0.197</td><td char="." align="char">0.619</td><td char="." align="char">51.2</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">58.8</td><td char="." align="char">55.7</td><td char="." align="char">56.5</td><td char="." align="char">0.613</td><td char="." align="char">0.507</td><td char="." align="char">0.031</td><td char="." align="char">57.2</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">67.2</td><td char="." align="char">91.9</td><td char="." align="char">85.7</td><td char="." align="char">0.796</td><td char="." align="char">0.591</td><td char="." align="char">&#x02212;0.247</td><td char="." align="char">78.5</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">72.3</td><td char="." align="char">93.0</td><td char="." align="char">87.8</td><td char="." align="char">0.082</td><td char="." align="char">0.653</td><td char="." align="char">&#x02212;0.207</td><td char="." align="char">81.9</td></tr><tr><td align="left" colspan="8">Undersampling (1:4) training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">88.2</td><td char="." align="char">31.1</td><td char="." align="char">42.5</td><td char="." align="char">0.694</td><td char="." align="char">0.193</td><td char="." align="char">0.571</td><td char="." align="char">52.37</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">64.7</td><td char="." align="char">73.5</td><td char="." align="char">71.8</td><td char="." align="char">0.731</td><td char="." align="char">0.382</td><td char="." align="char">&#x02212;0.088</td><td char="." align="char">68.9</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">63.0</td><td char="." align="char">92.4</td><td char="." align="char">86.6</td><td char="." align="char">0.777</td><td char="." align="char">0.554</td><td char="." align="char">&#x02212;0.294</td><td char="." align="char">76.2</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">68.9</td><td char="." align="char">94.7</td><td char="." align="char">89.6</td><td char="." align="char">0.823</td><td char="." align="char">0.636</td><td char="." align="char">&#x02212;0.258</td><td char="." align="char">80.7</td></tr><tr><td align="left" colspan="8">Undersampling (1:5) training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">89.1</td><td char="." align="char">31.1</td><td char="." align="char">40.8</td><td char="." align="char">0.692</td><td char="." align="char">0.202</td><td char="." align="char">0.58</td><td char="." align="char">52.6</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">66.4</td><td char="." align="char">79.0</td><td char="." align="char">76.9</td><td char="." align="char">0.791</td><td char="." align="char">0.454</td><td char="." align="char">&#x02212;0.126</td><td char="." align="char">72.42</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">57.1</td><td char="." align="char">94.6</td><td char="." align="char">88.4</td><td char="." align="char">0.759</td><td char="." align="char">0.61</td><td char="." align="char">&#x02212;0.375</td><td char="." align="char">73.4</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">70.6</td><td char="." align="char">93.9</td><td char="." align="char">90.1</td><td char="." align="char">0.841</td><td char="." align="char">0.645</td><td char="." align="char">&#x02212;0.233</td><td char="." align="char">81.4</td></tr><tr><td align="left" colspan="8">Undersampling (1:6) training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">89.1</td><td char="." align="char">29.6</td><td char="." align="char">38.1</td><td char="." align="char">0.688</td><td char="." align="char">0.187</td><td char="." align="char">0.595</td><td char="." align="char">51.3</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">68.1</td><td char="." align="char">80.4</td><td char="." align="char">78.6</td><td char="." align="char">0.805</td><td char="." align="char">0.485</td><td char="." align="char">&#x02212;0.123</td><td char="." align="char">73.9</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">56.3</td><td char="." align="char">95.0</td><td char="." align="char">89.4</td><td char="." align="char">0.756</td><td char="." align="char">0.513</td><td char="." align="char">&#x02212;0.387</td><td char="." align="char">73.13</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">71.4</td><td char="." align="char">95.2</td><td char="." align="char">91.8</td><td char="." align="char">0.824</td><td char="." align="char">0.666</td><td char="." align="char">&#x02212;0.238</td><td char="." align="char">82.4</td></tr></tbody></table></table-wrap>
</p></sec><sec id="Sec17"><title>Learning performance on SMOTE oversampled datasets</title><p>SMO performed better than all the other machine learning algorithms closely followed by FLDA on SMOTE oversampled datasets. The values of different performance parameters are recorded in the Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>. One of the best noticeable effects of oversampling is the immediate increase in sensitivity values for all the four machine learning algorithms. There is a regular increasing trend for the Youden&#x02019;s Index (which shows the model&#x02019;s ability to avoid faults) with increasing rate of SMOTE oversampling. The best trade-off for the different evaluation parameters was obtained for the SMOTE 500&#x000a0;% dataset with SMO as the machine learning algorithm. This particular training dataset gave the best performance evaluation metrics with SMO as the learning algorithm. With this training dataset we could achieve 98.5&#x000a0;% sensitivity, 92.6&#x000a0;% specificity, 95.3&#x000a0;% overall accuracy, and 0.955 of AUC. A high value of sensitivity indicates that the model is very accurate for the positive minority class samples. A positive dominance index of 0.059 also confirms the fact that the model is good in predicting minority samples. A high value of the Youden&#x02019;s Index (0.911) indicates the model&#x02019;s superiority in fault avoidance ability. A <italic>g</italic>-means value of 95.5 also indicates an optimal balance between sensitivity and specificity. ROC plots for the four different machine learning algorithms trained on the best performing training set (SMOTE oversampled 500&#x000a0;% dataset) are shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<table-wrap id="Tab4"><label>Table&#x000a0;4</label><caption><p>Performance evaluation metrics of the different machine learning algorithms trained on the different SMOTE oversampled training sets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning Algorithms</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">Accuracy</th><th align="left">AUC</th><th align="left">Youden&#x02019;s Index</th><th align="left">Dominance</th><th align="left">
<italic>g</italic>-means</th></tr></thead><tbody><tr><td align="left" colspan="8">SMOTE 100&#x000a0;% training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">91.2</td><td char="." align="char">33.1</td><td char="." align="char">46.1</td><td char="." align="char">0.738</td><td char="." align="char">0.243</td><td char="." align="char">0.581</td><td char="." align="char">54.9</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">81.5</td><td char="." align="char">84.5</td><td char="." align="char">83.8</td><td char="." align="char">0.896</td><td char="." align="char">0.660</td><td char="." align="char">&#x02212;0.030</td><td char="." align="char">82.9</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">81.1</td><td char="." align="char">94.7</td><td char="." align="char">91.6</td><td char="." align="char">0.879</td><td char="." align="char">0.758</td><td char="." align="char">&#x02212;0.136</td><td char="." align="char">87.6</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">97.9</td><td char="." align="char">85.1</td><td char="." align="char">88.0</td><td char="." align="char">0.912</td><td char="." align="char">0.830</td><td char="." align="char">0.128</td><td char="." align="char">91.2</td></tr><tr><td align="left" colspan="8">SMOTE 200&#x000a0;% training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">91.6</td><td char="." align="char">35.0</td><td char="." align="char">52.1</td><td char="." align="char">0.749</td><td char="." align="char">0.266</td><td char="." align="char">0.566</td><td char="." align="char">56.6</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">91.3</td><td char="." align="char">85.4</td><td char="." align="char">87.2</td><td char="." align="char">0.934</td><td char="." align="char">0.767</td><td char="." align="char">0.005</td><td char="." align="char">88.3</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">92.4</td><td char="." align="char">93.9</td><td char="." align="char">93.5</td><td char="." align="char">0.932</td><td char="." align="char">0.863</td><td char="." align="char">&#x02212;0.015</td><td char="." align="char">93.1</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">98.9</td><td char="." align="char">79.7</td><td char="." align="char">85.5</td><td char="." align="char">0.894</td><td char="." align="char">0.786</td><td char="." align="char">0.192</td><td char="." align="char">88.7</td></tr><tr><td align="left" colspan="8">SMOTE 300&#x000a0;% training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">91.2</td><td char="." align="char">36.0</td><td char="." align="char">56.1</td><td char="." align="char">0.751</td><td char="." align="char">0.272</td><td char="." align="char">0.552</td><td char="." align="char">57.2</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">95.2</td><td char="." align="char">84.4</td><td char="." align="char">88.3</td><td char="." align="char">0.946</td><td char="." align="char">0.796</td><td char="." align="char">0.108</td><td char="." align="char">89.6</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">96.2</td><td char="." align="char">92.3</td><td char="." align="char">93.7</td><td char="." align="char">0.942</td><td char="." align="char">0.885</td><td char="." align="char">0.003</td><td char="." align="char">94.2</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">99.4</td><td char="." align="char">79.1</td><td char="." align="char">86.5</td><td char="." align="char">0.890</td><td char="." align="char">0.785</td><td char="." align="char">0.203</td><td char="." align="char">88.6</td></tr><tr><td align="left" colspan="8">SMOTE 400&#x000a0;% training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">90.9</td><td char="." align="char">36.9</td><td char="." align="char">56.1</td><td char="." align="char">0.751</td><td char="." align="char">0.278</td><td char="." align="char">0.54</td><td char="." align="char">57.9</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">95.8</td><td char="." align="char">84.9</td><td char="." align="char">89.4</td><td char="." align="char">0.952</td><td char="." align="char">0.807</td><td char="." align="char">0.109</td><td char="." align="char">90.1</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">96.5</td><td char="." align="char">91.8</td><td char="." align="char">93.7</td><td char="." align="char">0.941</td><td char="." align="char">0.883</td><td char="." align="char">0.047</td><td char="." align="char">94.1</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">99.3</td><td char="." align="char">74.6</td><td char="." align="char">84.9</td><td char="." align="char">0.870</td><td char="." align="char">0.733</td><td char="." align="char">0.247</td><td char="." align="char">86.0</td></tr><tr><td align="left" colspan="8">SMOTE 500&#x000a0;% training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">92.0</td><td char="." align="char">36.8</td><td char="." align="char">62.4</td><td char="." align="char">0.745</td><td char="." align="char">0.288</td><td char="." align="char">0.552</td><td char="." align="char">58.1</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">97.3</td><td char="." align="char">83.7</td><td char="." align="char">90.0</td><td char="." align="char">0.962</td><td char="." align="char">0.810</td><td char="." align="char">0.136</td><td char="." align="char">90.2</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">98.5</td><td char="." align="char">92.6</td><td char="." align="char">95.3</td><td char="." align="char">0.955</td><td char="." align="char">0.911</td><td char="." align="char">0.059</td><td char="." align="char">95.5</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">99.6</td><td char="." align="char">73.8</td><td char="." align="char">85.8</td><td char="." align="char">0.867</td><td char="." align="char">0.734</td><td char="." align="char">0.258</td><td char="." align="char">85.7</td></tr><tr><td align="left" colspan="8">SMOTE 594&#x000a0;% (fully balanced) training set</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">92.4</td><td char="." align="char">36.4</td><td char="." align="char">64.4</td><td char="." align="char">0.742</td><td char="." align="char">0.288</td><td char="." align="char">0.56</td><td char="." align="char">57.9</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">97.7</td><td char="." align="char">85.1</td><td char="." align="char">91.4</td><td char="." align="char">0.964</td><td char="." align="char">0.828</td><td char="." align="char">0.12</td><td char="." align="char">91.1</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">97.9</td><td char="." align="char">90.8</td><td char="." align="char">94.4</td><td char="." align="char">0.944</td><td char="." align="char">0.887</td><td char="." align="char">0.071</td><td char="." align="char">94.2</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">99.6</td><td char="." align="char">73.5</td><td char="." align="char">86.6</td><td char="." align="char">0.862</td><td char="." align="char">0.731</td><td char="." align="char">0.261</td><td char="." align="char">85.5</td></tr></tbody></table></table-wrap>
<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>ROC curves of the four classifiers using the training set with optimal class distribution [SMOTE (500&#x000a0;%)]</p></caption><graphic xlink:href="13205_2016_410_Fig2_HTML" id="MO11"/></fig>
</p><p>To further validate the learned models trained on a SMOTE oversampled dataset (500&#x000a0;%), we have used leave on out cross validation test (Chou and Zhang <xref ref-type="bibr" rid="CR12">1995</xref>). It is deemed as the most objective and robust test and has been used by many researchers for the assessment of classifier models (Chou and Cai <xref ref-type="bibr" rid="CR11">2004</xref>; Gao et al. <xref ref-type="bibr" rid="CR15">2005</xref>; Xie et al. <xref ref-type="bibr" rid="CR51">2013</xref>), the results are given in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5"><label>Table&#x000a0;5</label><caption><p>Leave on out cross validation performance evaluation metrics on the best training set</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning algorithms</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">Accuracy</th><th align="left">AUC</th><th align="left">Youden&#x02019;s Index</th><th align="left">Dominance</th><th align="left">
<italic>g</italic>-means</th></tr></thead><tbody><tr><td align="left" colspan="8">LOOCV on SMOTE (500&#x000a0;%)</td></tr><tr><td align="left">&#x000a0;NB</td><td char="." align="char">92.3</td><td char="." align="char">36.4</td><td char="." align="char">62.3</td><td char="." align="char">0.745</td><td char="." align="char">0.287</td><td char="." align="char">0.559</td><td char="." align="char">57.96</td></tr><tr><td align="left">&#x000a0;FLDA</td><td char="." align="char">97.2</td><td char="." align="char">85.1</td><td char="." align="char">90.7</td><td char="." align="char">0.966</td><td char="." align="char">0.823</td><td char="." align="char">0.121</td><td char="." align="char">90.90</td></tr><tr><td align="left">&#x000a0;SMO</td><td char="." align="char">98.9</td><td char="." align="char">92.3</td><td char="." align="char">95.3</td><td char="." align="char">0.956</td><td char="." align="char">0.912</td><td char="." align="char">0.066</td><td char="." align="char">95.50</td></tr><tr><td align="left">&#x000a0;IBK</td><td char="." align="char">99.4</td><td char="." align="char">75.8</td><td char="." align="char">86.8</td><td char="." align="char">0.876</td><td char="." align="char">0.752</td><td char="." align="char">0.236</td><td char="." align="char">86.80</td></tr></tbody></table></table-wrap>
</p><p>Further, a corrected resampled paired <italic>t</italic> test was performed using WEKA with SMO as the baseline classifier. The <italic>t</italic> test was performed at the 5&#x000a0;% significance level. Each tenfold cross validation was repeated ten times (10&#x000a0;&#x000d7;&#x000a0;10 runs for each algorithm). Percentage correctly predicted instances, AUC, TP rate and TN rate was used for comparison with <italic>t</italic> test. The results of the <italic>t</italic> test are provided in the supplementary material (Table S4a&#x02013;d).</p></sec><sec id="Sec18"><title>Comparing the results with previous study</title><p>We have compared the evaluation metric of the current study with the previous study and the performance evaluation metric values for the current best training set and the previously reported values are presented in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab6"><label>Table&#x000a0;6</label><caption><p>Comparison of the performance evaluation metrics of the current work with the previous methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">Accuracy</th><th align="left">AUC</th><th align="left">Youden&#x02019;s Index</th><th align="left">Dominance</th><th align="left">
<italic>g</italic>-means</th></tr></thead><tbody><tr><td align="left">Jagga and Gupta (<xref ref-type="bibr" rid="CR20">2014</xref>)</td><td char="." align="char">80.90</td><td char="." align="char">80.57</td><td char="." align="char">80.61</td><td char="." align="char">0.910</td><td char="." align="char">0.614</td><td char="." align="char">0.003</td><td char="." align="char">80.70</td></tr><tr><td align="left">SMO [SMOTE (500&#x000a0;%)]</td><td char="." align="char">98.50</td><td char="." align="char">92.60</td><td char="." align="char">95.30</td><td char="." align="char">0.955</td><td char="." align="char">0.911</td><td char="." align="char">0.059</td><td char="." align="char">95.50</td></tr></tbody></table></table-wrap>
</p><p>On comparison with the previous method, the current SMOTE (500&#x000a0;%) model achieved far better performance evaluation metrics.</p><p>It is also observed that both the SMOTE oversampling and random undersampling have least effect on the performance of the naive Bayes algorithm, a similar observation has also been made by (Daskalaki et al. <xref ref-type="bibr" rid="CR13">2006</xref>).</p></sec><sec id="Sec19"><title>Characterization of RNA-silencing suppressors using sequence-based features</title><p>In Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, we have plotted the heat map representation of the sequence attributes except the dipeptides. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> presents the heat map representation of the dipeptides. The color bar in both the figures (on the right side of both the figures) shows the color intensity proportional to the feature ranking scores which are calculated according to their discriminating ability. Observing the Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, arginine, polar and nonpolar property groups are the most useful discriminatory features. From Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, it can also be observed that DF, SF, NN, DT, CW, CG are the most discriminatory dipeptides.<fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>Heat map representation of ranking the sequence features (excluding dipeptides) according to their discriminative ability</p></caption><graphic xlink:href="13205_2016_410_Fig3_HTML" id="MO12"/></fig>
<fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>Heat map representation of ranking the dipeptides according to their discriminative ability</p></caption><graphic xlink:href="13205_2016_410_Fig4_HTML" id="MO13"/></fig>
</p><p>Arginines are relatively important in binding sites (Barnes <xref ref-type="bibr" rid="CR3">2007</xref>), also it is imperative to mention the importance of the role of arginine in suppressor activity of PRS suppressor (2b) of a cucumber mosaic virus strain (CM95R) (Goto et al. <xref ref-type="bibr" rid="CR17">2007</xref>) where it facilitates in binding to RNA and in potato virus M where mutational studies have shown the importance of arginines in suppression activity (Senshu et al. <xref ref-type="bibr" rid="CR42">2011</xref>). The importance of nonpolar amino acids, specifically isoleucine in suppression activity is also emphasized in (Carr and Pathology <xref ref-type="bibr" rid="CR8">2007</xref>).</p></sec></sec><sec id="Sec20"><title>Conclusions</title><p>Machine learning-based approaches are apposite techniques when compared to sequence alignment-based methods for the prediction of plant virus-encoded RNA-silencing suppressors and can become the superior alternative if the imbalance dataset problem is properly resolved. The protein family classification problem intrinsically presents a class imbalance situation, where the class of interest is a particular protein family which constitutes the positive class and the rest of the protein families belonging to the negative classes. Naturally, there is a large difference between the number of instances belonging to positive and negative classes. Depending on the mathematical representation of the protein sequences, machine learning-based approaches can capture the hidden relationship among the calculated protein attributes, which is most of the times better than alignment-based methods for protein classification. The plant virus-encoded RNA-silencing suppressor protein classification presents a data imbalance problem; we compared the learning of different machine learning algorithms on imbalanced, SMOTE oversampled and randomly undersampled datasets. The results reported in this study showed that learning is non-optimal for imbalanced positive and negative class data sets. The behavior of the machine learning algorithms is different in SMOTE oversampling and random undersampling. IBK performed better on randomly undersampled datasets, while the performance of SMO is superior to all other machine learning algorithms on SMOTE oversampled datasets. Better performance evaluation metrics were obtained on SMOTE oversampled datasets than on the randomly undersampled datasets. The best model is achieved with SMOTE oversampling when SMO is used as the learning algorithm. This also points to the fact that the full (ideal) balancing between the positive and negative classes may not fully eliminate the classifier bias. The current study supports and provides evidence to the fact that the learning of different machine learning algorithms can be improved using an optimal class distribution and also the fully balanced class distribution need not be optimal for the training of the learning algorithms. Individual accuracies and learning on the positive and negative classes can be increased by changing the class distribution. Overall the performance of the various machine learning algorithms on SMOTE oversampled datasets is better than the random undersampled datasets. Further, we have ranked the calculated sequence features according to their discriminating ability in classifying plant virus-encoded RNA-silencing suppressors from non-suppressors. The current pipeline can be successfully applied to other protein family classification problem with different degrees of imbalance. The current method explored the possibility of improvement in prediction accuracy of the four machine learning algorithms using an optimal class distribution that provides the best trade-off between imbalance dataset and the diversity of the dataset. A comprehensive study was carried out and presented in detail the behavior of the tested learning algorithms with varying degrees of resampling. It is also proved that prediction accuracy for the plant virus suppressor proteins can be improved using the optimal class distribution ratio.</p><p>Future research can be carried out by incorporating additional diversifying techniques to deal with the related problem of incomplete learning. More sophisticated techniques can be evolved to deal with the trade-off between the balancing factor and input instance diversity. Further research in this direction can lead to the formulation of some kind of standard in creating benchmark data sets to every specific biological problem.</p></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec21"><p>Below is the link to the electronic supplementary material.
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="13205_2016_410_MOESM1_ESM.docx"><caption><p>Supplementary material 1 (DOCX 19&#x000a0;kb)</p></caption></media></supplementary-material>
</p></sec></sec></body><back><ack><p>The authors are very grateful to Department of Computer Science, Faculty of Science, Banaras Hindu University for supports in this study.</p></ack><notes notes-type="COI-statement"><title>Compliance with ethical standards</title><sec id="FPar1"><title>Conflict of interest</title><p>The authors declare that there are no conflicts of interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>SF</given-names></name><name><surname>Gish</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Myers</surname><given-names>EW</given-names></name><name><surname>Lipman</surname><given-names>DJ</given-names></name></person-group><article-title>Basic local alignment search tool</article-title><source>J Mol Biol</source><year>1990</year><volume>215</volume><fpage>403</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/S0022-2836(05)80360-2</pub-id><pub-id pub-id-type="pmid">2231712</pub-id></element-citation></ref><ref id="CR2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>S</given-names></name><name><surname>Madden</surname><given-names>T</given-names></name><name><surname>Schaffer</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Lipman</surname><given-names>D</given-names></name></person-group><article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title><source>Nucleic Acids Res</source><year>1997</year><volume>25</volume><fpage>3389</fpage><lpage>3402</lpage><pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id><pub-id pub-id-type="pmid">9254694</pub-id></element-citation></ref><ref id="CR3"><mixed-citation publication-type="other">Barnes MR (2007) Bioinformatics for geneticists: a bioinformatics primer for the analysis of genetic data. Wiley</mixed-citation></ref><ref id="CR4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barua</surname><given-names>S</given-names></name><name><surname>Islam</surname><given-names>MM</given-names></name><name><surname>Xin</surname><given-names>Y</given-names></name><name><surname>Murase</surname><given-names>K</given-names></name></person-group><article-title>MWMOTE&#x02014;majority weighted minority oversampling technique for imbalanced data set learning knowledge and data engineering</article-title><source>IEEE Trans</source><year>2014</year><volume>26</volume><fpage>405</fpage><lpage>425</lpage></element-citation></ref><ref id="CR5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batuwita</surname><given-names>R</given-names></name><name><surname>Palade</surname><given-names>V</given-names></name></person-group><article-title>microPred: effective classification of pre-miRNAs for human miRNA gene prediction</article-title><source>Bioinformatics</source><year>2009</year><volume>25</volume><fpage>989</fpage><lpage>995</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp107</pub-id><pub-id pub-id-type="pmid">19233894</pub-id></element-citation></ref><ref id="CR6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blagus</surname><given-names>R</given-names></name><name><surname>Lusa</surname><given-names>L</given-names></name></person-group><article-title>SMOTE for high-dimensional class-imbalanced data</article-title><source>BMC Bioinform</source><year>2013</year><volume>14</volume><fpage>106</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-14-106</pub-id></element-citation></ref><ref id="CR7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>AP</given-names></name></person-group><article-title>The use of the area under the ROC curve in the evaluation of machine learning algorithms</article-title><source>Pattern Recogn</source><year>1997</year><volume>30</volume><fpage>1145</fpage><lpage>1159</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(96)00142-2</pub-id></element-citation></ref><ref id="CR8"><mixed-citation publication-type="other">Carr T, Pathology ISUP (2007) Genetic and molecular investigation of compatible plant-virus interactions. Iowa State University, Iowa</mixed-citation></ref><ref id="CR9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chapman</surname><given-names>EJ</given-names></name><name><surname>Prokhnevsky</surname><given-names>AI</given-names></name><name><surname>Gopinath</surname><given-names>K</given-names></name><name><surname>Dolja</surname><given-names>VV</given-names></name><name><surname>Carrington</surname><given-names>JC</given-names></name></person-group><article-title>Viral RNA silencing suppressors inhibit the microRNA pathway at an intermediate step</article-title><source>Genes Dev</source><year>2004</year><volume>18</volume><fpage>1179</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1101/gad.1201204</pub-id><pub-id pub-id-type="pmid">15131083</pub-id></element-citation></ref><ref id="CR10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chawla</surname><given-names>NV</given-names></name><name><surname>Bowyer</surname><given-names>KW</given-names></name><name><surname>Hall</surname><given-names>LO</given-names></name><name><surname>Kegelmeyer</surname><given-names>WP</given-names></name></person-group><article-title>SMOTE: synthetic minority over-sampling technique</article-title><source>J Artif Int Res</source><year>2002</year><volume>16</volume><fpage>321</fpage><lpage>357</lpage></element-citation></ref><ref id="CR11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K-C</given-names></name><name><surname>Cai</surname><given-names>Y-D</given-names></name></person-group><article-title>Predicting protein structural class by functional domain composition</article-title><source>Biochem Biophys Res Commun</source><year>2004</year><volume>321</volume><fpage>1007</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/j.bbrc.2004.07.059</pub-id><pub-id pub-id-type="pmid">15358128</pub-id></element-citation></ref><ref id="CR12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name></person-group><article-title>Prediction of protein structural classes</article-title><source>Crit Rev Biochem Mol Biol</source><year>1995</year><volume>30</volume><fpage>275</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.3109/10409239509083488</pub-id><pub-id pub-id-type="pmid">7587280</pub-id></element-citation></ref><ref id="CR13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daskalaki</surname><given-names>S</given-names></name><name><surname>Kopanas</surname><given-names>I</given-names></name><name><surname>Avouris</surname><given-names>NM</given-names></name></person-group><article-title>Evaluation of classifiers for an uneven class distribution problem</article-title><source>Appl Artif Intell</source><year>2006</year><volume>20</volume><fpage>381</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1080/08839510500313653</pub-id></element-citation></ref><ref id="CR14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunoyer</surname><given-names>P</given-names></name><name><surname>Lecellier</surname><given-names>CH</given-names></name><name><surname>Parizotto</surname><given-names>EA</given-names></name><name><surname>Himber</surname><given-names>C</given-names></name><name><surname>Voinnet</surname><given-names>O</given-names></name></person-group><article-title>Probing the microRNA and small interfering RNA pathways with virus-encoded suppressors of RNA silencing</article-title><source>Plant Cell</source><year>2004</year><volume>16</volume><fpage>1235</fpage><lpage>1250</lpage><pub-id pub-id-type="doi">10.1105/tpc.020719</pub-id><pub-id pub-id-type="pmid">15084715</pub-id></element-citation></ref><ref id="CR15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Shao</surname><given-names>S</given-names></name><name><surname>Xiao</surname><given-names>X</given-names></name><name><surname>Ding</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Chou</surname><given-names>KC</given-names></name></person-group><article-title>Using pseudo amino acid composition to predict protein subcellular location: approached with Lyapunov Index, Bessel function, and Chebyshev filter</article-title><source>Amino Acids</source><year>2005</year><volume>28</volume><fpage>373</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1007/s00726-005-0206-9</pub-id><pub-id pub-id-type="pmid">15889221</pub-id></element-citation></ref><ref id="CR16"><mixed-citation publication-type="other">Garc&#x000ed;a V, Mollineda RA, S&#x000e1;nchez JS (2009) Index of balanced accuracy: a performance measure for skewed class distributions. In: Araujo H, Mendon&#x000e7;a A, Pinho A, Torres M (eds) Pattern recognition and image analysis, vol 5524. Lecture notes in computer science. Springer, Heidelberg, pp 441&#x02013;448. doi:10.1007/978-3-642-02172-5_57</mixed-citation></ref><ref id="CR17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goto</surname><given-names>K</given-names></name><name><surname>Kobori</surname><given-names>T</given-names></name><name><surname>Kosaka</surname><given-names>Y</given-names></name><name><surname>Natsuaki</surname><given-names>T</given-names></name><name><surname>Masuta</surname><given-names>C</given-names></name></person-group><article-title>Characterization of silencing suppressor 2b of cucumber mosaic virus based on examination of its small RNA-binding abilities</article-title><source>Plant Cell Physiol</source><year>2007</year><volume>48</volume><fpage>1050</fpage><lpage>1060</lpage><pub-id pub-id-type="doi">10.1093/pcp/pcm074</pub-id><pub-id pub-id-type="pmid">17567638</pub-id></element-citation></ref><ref id="CR18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>M</given-names></name><name><surname>Frank</surname><given-names>E</given-names></name><name><surname>Holmes</surname><given-names>G</given-names></name><name><surname>Pfahringer</surname><given-names>B</given-names></name><name><surname>Reutemann</surname><given-names>P</given-names></name><name><surname>Witten</surname><given-names>IH</given-names></name></person-group><article-title>The WEKA data mining software: an update</article-title><source>SIGKDD Explor Newsl</source><year>2009</year><volume>11</volume><fpage>10</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1145/1656274.1656278</pub-id></element-citation></ref><ref id="CR19"><mixed-citation publication-type="other">Han H, Wang W-Y, Mao B-H (2005) Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning. In: Huang D-S, Zhang X-P, Huang G-B (eds) Advances in intelligent computing, vol 3644. Lecture notes in computer science. Springer, Heidelberg, pp 878&#x02013;887. doi:10.1007/11538059_91</mixed-citation></ref><ref id="CR20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jagga</surname><given-names>Z</given-names></name><name><surname>Gupta</surname><given-names>D</given-names></name></person-group><article-title>Supervised learning classification models for prediction of plant virus encoded RNA silencing suppressors</article-title><source>PLoS ONE</source><year>2014</year><volume>9</volume><fpage>e97446</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0097446</pub-id><pub-id pub-id-type="pmid">24828116</pub-id></element-citation></ref><ref id="CR21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kandaswamy</surname><given-names>K</given-names></name><name><surname>Pugalenthi</surname><given-names>G</given-names></name><name><surname>Hazrati</surname><given-names>M</given-names></name><name><surname>Kalies</surname><given-names>K-U</given-names></name><name><surname>Martinetz</surname><given-names>T</given-names></name></person-group><article-title>BLProt: prediction of bioluminescent proteins based on support vector machine and relief feature selection</article-title><source>BMC Bioinformatics</source><year>2011</year><volume>12</volume><fpage>345</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-12-345</pub-id><pub-id pub-id-type="pmid">21849049</pub-id></element-citation></ref><ref id="CR22"><mixed-citation publication-type="other">Kira K, Rendell LA (1992) A practical approach to feature selection. Paper presented at the proceedings of the ninth international workshop on machine learning, Aberdeen</mixed-citation></ref><ref id="CR23"><mixed-citation publication-type="other">Kubat M, Holte R, Matwin S (1997) Learning when negative examples abound. In: van Someren M, Widmer G (eds) Machine learning: ECML-97, vol 1224. Lecture notes in computer science. Springer, Heidelberg, pp 146&#x02013;153. doi:10.1007/3-540-62858-4_79</mixed-citation></ref><ref id="CR24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumari</surname><given-names>P</given-names></name><name><surname>Nath</surname><given-names>A</given-names></name><name><surname>Chaube</surname><given-names>R</given-names></name></person-group><article-title>Identification of human drug targets using machine-learning algorithms</article-title><source>Comp Biomed</source><year>2015</year><volume>56</volume><fpage>175</fpage><lpage>181</lpage></element-citation></ref><ref id="CR25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>PH</given-names></name></person-group><article-title>Resampling methods improve the predictive power of modeling in class-imbalanced datasets</article-title><source>Int J Environ Res Public Health</source><year>2014</year><volume>11</volume><fpage>9776</fpage><lpage>9789</lpage><pub-id pub-id-type="doi">10.3390/ijerph110909776</pub-id><pub-id pub-id-type="pmid">25238271</pub-id></element-citation></ref><ref id="CR26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Godzik</surname><given-names>A</given-names></name></person-group><article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>1658</fpage><lpage>1659</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id><pub-id pub-id-type="pmid">16731699</pub-id></element-citation></ref><ref id="CR27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Huang</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name></person-group><article-title>Suppression of RNA silencing by a plant DNA virus satellite requires a host calmodulin-like protein to repress <italic>RDR6</italic> expression</article-title><source>PLoS Pathog</source><year>2014</year><volume>10</volume><fpage>e1003921</fpage><pub-id pub-id-type="doi">10.1371/journal.ppat.1003921</pub-id><pub-id pub-id-type="pmid">24516387</pub-id></element-citation></ref><ref id="CR28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Pi</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name></person-group><article-title>The prediction of protein-protein interaction sites based on RBF classifier improved by SMOTE</article-title><source>Math Probl Eng</source><year>2014</year><volume>2014</volume><fpage>7</fpage></element-citation></ref><ref id="CR29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Jin</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Replication-associated proteins encoded by wheat dwarf virus act as RNA silencing suppressors</article-title><source>Virus Res</source><year>2014</year><volume>190</volume><fpage>34</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.virusres.2014.06.014</pub-id><pub-id pub-id-type="pmid">25016035</pub-id></element-citation></ref><ref id="CR30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacIsaac</surname><given-names>KD</given-names></name><etal/></person-group><article-title>A hypothesis-based approach for identifying the binding specificity of regulatory proteins from chromatin immunoprecipitation data</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>423</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bti815</pub-id><pub-id pub-id-type="pmid">16332710</pub-id></element-citation></ref><ref id="CR31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishra</surname><given-names>NK</given-names></name><name><surname>Chang</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>PX</given-names></name></person-group><article-title>Prediction of membrane transport proteins and their substrate specificities using primary sequence information</article-title><source>PLoS ONE</source><year>2014</year><volume>9</volume><fpage>e100278</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0100278</pub-id><pub-id pub-id-type="pmid">24968309</pub-id></element-citation></ref><ref id="CR32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakamura</surname><given-names>M</given-names></name><name><surname>Kajiwara</surname><given-names>Y</given-names></name><name><surname>Otsuka</surname><given-names>A</given-names></name><name><surname>Kimura</surname><given-names>H</given-names></name></person-group><article-title>LVQ-SMOTE&#x02014;learning vector quantization based synthetic minority over-sampling technique for biomedical data</article-title><source>BioData Min</source><year>2013</year><volume>6</volume><fpage>16</fpage><pub-id pub-id-type="doi">10.1186/1756-0381-6-16</pub-id><pub-id pub-id-type="pmid">24088532</pub-id></element-citation></ref><ref id="CR33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>A</given-names></name><name><surname>Subbiah</surname><given-names>K</given-names></name></person-group><article-title>Inferring biological basis about psychrophilicity by interpreting the rules generated from the correctly classified input instances by a classifier</article-title><source>Comput Biol Chem</source><year>2014</year><volume>53</volume><fpage>198</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.compbiolchem.2014.10.002</pub-id><pub-id pub-id-type="pmid">25462328</pub-id></element-citation></ref><ref id="CR34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>A</given-names></name><name><surname>Subbiah</surname><given-names>K</given-names></name></person-group><article-title>Maximizing lipocalin prediction through balanced and diversified training set and decision fusion</article-title><source>Comput Biol Chem</source><year>2015</year><volume>59</volume><fpage>101</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.compbiolchem.2015.09.011</pub-id><pub-id pub-id-type="pmid">26433483</pub-id></element-citation></ref><ref id="CR35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>A</given-names></name><name><surname>Subbiah</surname><given-names>K</given-names></name></person-group><article-title>Unsupervised learning assisted robust prediction of bioluminescent proteins</article-title><source>Comput Biol Med</source><year>2015</year><volume>68</volume><fpage>27</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2015.10.013</pub-id><pub-id pub-id-type="pmid">26599828</pub-id></element-citation></ref><ref id="CR36"><mixed-citation publication-type="other">Nath A, Chaube R, Karthikeyan S (2012) Discrimination of psychrophilic and mesophilic proteins using random forest algorithm. In: Biomedical engineering and biotechnology (iCBEB), 2012 international conference, 28&#x02013;30 May 2012, pp 179&#x02013;182. doi:10.1109/iCBEB.2012.151</mixed-citation></ref><ref id="CR37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>A</given-names></name><name><surname>Chaube</surname><given-names>R</given-names></name><name><surname>Subbiah</surname><given-names>K</given-names></name></person-group><article-title>An insight into the molecular basis for convergent evolution in fish antifreeze proteins</article-title><source>Comput Biol Med</source><year>2013</year><volume>43</volume><fpage>817</fpage><lpage>821</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2013.04.013</pub-id><pub-id pub-id-type="pmid">23746722</pub-id></element-citation></ref><ref id="CR38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>P&#x000e9;rez-Ca&#x000f1;am&#x000e1;s</surname><given-names>M</given-names></name><name><surname>Hern&#x000e1;ndez</surname><given-names>C</given-names></name></person-group><article-title>Key importance of small RNA binding for the activity of a glycine/tryptophan (GW) motif-containing viral suppressor of RNA silencing</article-title><source>J Biol Chem</source><year>2014</year></element-citation></ref><ref id="CR39"><mixed-citation publication-type="other">Platt JC (1999) Fast training of support vector machines using sequential minimal optimization. In: Advances in kernel methods. MIT Press, pp 185&#x02013;208</mixed-citation></ref><ref id="CR40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pugalenthi</surname><given-names>G</given-names></name><name><surname>Kandaswamy</surname><given-names>KK</given-names></name><name><surname>Suganthan</surname><given-names>PN</given-names></name><name><surname>Archunan</surname><given-names>G</given-names></name><name><surname>Sowdhamini</surname><given-names>R</given-names></name></person-group><article-title>Identification of functionally diverse lipocalin proteins from sequence information using support vector machine</article-title><source>Amino Acids</source><year>2010</year><volume>39</volume><fpage>777</fpage><lpage>783</lpage><pub-id pub-id-type="doi">10.1007/s00726-010-0520-8</pub-id><pub-id pub-id-type="pmid">20186553</pub-id></element-citation></ref><ref id="CR41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qu</surname><given-names>F</given-names></name><name><surname>Morris</surname><given-names>TJ</given-names></name></person-group><article-title>Suppressors of RNA silencing encoded by plant viruses and their role in viral infections</article-title><source>FEBS Lett</source><year>2005</year><volume>579</volume><fpage>5958</fpage><lpage>5964</lpage><pub-id pub-id-type="doi">10.1016/j.febslet.2005.08.041</pub-id><pub-id pub-id-type="pmid">16162340</pub-id></element-citation></ref><ref id="CR42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senshu</surname><given-names>H</given-names></name><etal/></person-group><article-title>A dual strategy for the suppression of host antiviral silencing: two distinct suppressors for viral replication and viral movement encoded by potato virus M</article-title><source>J Virol</source><year>2011</year><volume>85</volume><fpage>10269</fpage><lpage>10278</lpage><pub-id pub-id-type="doi">10.1128/JVI.05273-11</pub-id><pub-id pub-id-type="pmid">21752911</pub-id></element-citation></ref><ref id="CR43"><mixed-citation publication-type="other">Suvarna Vani K, Durga Bhavani S (2013) SMOTE based protein fold prediction classification. In: Meghanathan N, Nagamalai D, Chaki N (eds) Advances in computing and information technology, vol 177. Advances in intelligent systems and computing. Springer, Heidelberg, pp 541&#x02013;550. doi:10.1007/978-3-642-31552-7_55</mixed-citation></ref><ref id="CR44"><mixed-citation publication-type="other">Valli A, L&#x000f3;pez-Moya JJ, Garc&#x000ed;a JA (2001) RNA silencing and its suppressors in the plant-virus interplay. In: eLS. Wiley doi:10.1002/9780470015902.a0021261</mixed-citation></ref><ref id="CR45"><mixed-citation publication-type="other">Vapnik V (1995) The nature of statistical learning theory. Springer</mixed-citation></ref><ref id="CR46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><source>Statistical learning theory</source><year>1998</year><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="CR47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Dang</surname><given-names>M</given-names></name><name><surname>Hou</surname><given-names>H</given-names></name><name><surname>Mei</surname><given-names>Y</given-names></name><name><surname>Qian</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name></person-group><article-title>Identification of an RNA silencing suppressor encoded by a mastrevirus</article-title><source>J Gen Virol</source><year>2014</year><volume>95</volume><fpage>2082</fpage><lpage>2088</lpage><pub-id pub-id-type="doi">10.1099/vir.0.064246-0</pub-id><pub-id pub-id-type="pmid">24866851</pub-id></element-citation></ref><ref id="CR48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Q</given-names></name><name><surname>Dunbrack</surname><given-names>RL</given-names><suffix>Jr</suffix></name></person-group><article-title>the role of balanced training and testing data sets for binary classifiers in bioinformatics</article-title><source>PLoS ONE</source><year>2013</year><volume>8</volume><fpage>e67863</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0067863</pub-id><pub-id pub-id-type="pmid">23874456</pub-id></element-citation></ref><ref id="CR49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>GM</given-names></name><name><surname>Provost</surname><given-names>F</given-names></name></person-group><article-title>Learning when training data are costly: the effect of class distribution on tree induction</article-title><source>J Artif Int Res</source><year>2003</year><volume>19</volume><fpage>315</fpage><lpage>354</lpage></element-citation></ref><ref id="CR50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Fang</surname><given-names>Z</given-names></name><name><surname>Ma</surname><given-names>D</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>M</given-names></name></person-group><article-title>Identification of microRNA precursors based on random forest with network-level representation method of stem-loop structure</article-title><source>BMC Bioinformatics</source><year>2011</year><volume>12</volume><fpage>165</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-12-165</pub-id><pub-id pub-id-type="pmid">21575268</pub-id></element-citation></ref><ref id="CR51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>H-L</given-names></name><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Nie</surname><given-names>X-D</given-names></name></person-group><article-title>Using ensemble SVM to identify human GPCRs N-linked glycosylation sites based on the general form of Chou&#x02019;s PseAAC</article-title><source>Protein Eng Des Sel</source><year>2013</year><volume>26</volume><fpage>735</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1093/protein/gzt042</pub-id><pub-id pub-id-type="pmid">24048266</pub-id></element-citation></ref><ref id="CR52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Youden</surname><given-names>WJ</given-names></name></person-group><article-title>Index for rating diagnostic tests</article-title><source>Cancer</source><year>1950</year><volume>3</volume><fpage>32</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1002/1097-0142(1950)3:1&#x0003c;32::AID-CNCR2820030106&#x0003e;3.0.CO;2-3</pub-id><pub-id pub-id-type="pmid">15405679</pub-id></element-citation></ref></ref-list></back></article>